{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "neural_net.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AljcXYHE9DP5"
      },
      "source": [
        "# Assignment 02 Part 2: Neural Net Template\n",
        "\n",
        "This file contains the template code for the Neural Net with hidden layers.\n",
        "\n",
        "### Artificial Neural Net Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-4bUaDZYuW_",
        "outputId": "e74d5bf6-d8e5-4974-ca72-420b0bd742f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWztHPa8ZzC1",
        "outputId": "c7f7bdab-ab62-462f-c524-bdafb20790b2"
      },
      "source": [
        "cd drive/MyDrive/PhD/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/PhD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkQHMlvcbFBM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e18wSvdUv0ZN"
      },
      "source": [
        "def load_data():\n",
        "  train_data = np.array(pd.read_csv('mnist_test.csv'))\n",
        "  #train_data = np.array(train_data)\n",
        "  train_labels = train_data[:, 0]\n",
        "  train_data = train_data[:, 1:]\n",
        "  m, n = train_data.shape\n",
        "  train_x = train_data.reshape(m, n) # m number of samples, n number of features\n",
        "  train_y = train_labels.reshape(m, 1)\n",
        "  \n",
        "  test_data = np.array(pd.read_csv('mnist_train_10.csv'))\n",
        "  test_labels = test_data[:, 0]\n",
        "  test_data = test_data[:, 1:]\n",
        "  m1, n1 = test_data.shape\n",
        "  test_y = test_labels.reshape(m1, 1)\n",
        "  test_x = test_data.reshape(m1, n1)\n",
        "\n",
        "  sum = np.sum(train_x, axis=1, keepdims=True)\n",
        "  train_x = train_x / sum\n",
        "  sum = np.sum(test_x, axis=1, keepdims=True)\n",
        "  test_x = test_x/sum\n",
        " # print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
        "  return train_x, train_y, test_x, test_y\n",
        "  "
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrJm_d-l90-a"
      },
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])*0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csgb7bSlFvVx"
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    #print('number of layers: ', L)\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (X.shape[0], parameters['b' + str(L)].shape[0]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZotkA879Gcaf"
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (A_prev.shape[0], W.shape[0]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWnXUqVkGlxw"
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    Z = np.dot(A, W.T) + b.T       # W.dot(A) + b\n",
        "    \n",
        "    assert(Z.shape == ( A.shape[0], W.shape[0]))\n",
        "    cache = (A, W, b)\n",
        "    #print(A.shape, W.shape, b.shape, Z.shape)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9W40C8xGxpA"
      },
      "source": [
        "def sigmoid(Z):\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    assert(A.shape == Z.shape)\n",
        "    cache = Z \n",
        "    return A, cache\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPipK0ffGEOA"
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -np.sum(np.multiply(Y, np.log10(AL)) + np.multiply((1-Y), np.log(1-AL)))/m\n",
        "    #cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
        "   # cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "  #  assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbRStvULkLS9"
      },
      "source": [
        "def gradient(Z, activation):\n",
        "    if activation=='sigmoid':\n",
        "        A, _ = sigmoid(Z)\n",
        "        return A * (1 - A)\n",
        "    elif activation == 'relu':\n",
        "        A, _ = relu(Z)\n",
        "        return np.where(A>0, 1, 0)\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOuYKjw09DP-"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn import metrics\n",
        "class ANN:\n",
        "\n",
        "    #==========================================#\n",
        "    # The init method is called when an object #\n",
        "    # is created. It can be used to initialize #\n",
        "    # the attributes of the class.             #\n",
        "    #==========================================#\n",
        "    def __init__(self, no_inputs, no_hidden_layers=1, hidden_layer_size=28, max_iterations=20, learning_rate=0.1, no_outputs=10):\n",
        "\n",
        "        self.no_inputs = no_inputs\n",
        "        self.no_hidden_layers = no_hidden_layers\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.no_outputs = no_outputs\n",
        "        # TODO initialise weights\n",
        "\n",
        "        self.layer_dims = [self.no_inputs, 28, self.no_outputs]\n",
        "        self.parameters = initialize_parameters(self.layer_dims)\n",
        "\n",
        "        for i in range(1, len(self.layer_dims)):\n",
        "          print(self.parameters['W'+str(i)].shape)\n",
        "          print(self.parameters['b'+str(i)].shape)\n",
        "\n",
        "\n",
        "        self.max_iterations = max_iterations\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    #===================================#\n",
        "    # Performs the activation function. #\n",
        "    # Expects an array of values of     #\n",
        "    # shape (1,N) where N is the number #\n",
        "    # of nodes in the layer.            #\n",
        "    #===================================#\n",
        "   \n",
        "    #===============================#\n",
        "    # Trains the net using labelled #\n",
        "    # training data.                #\n",
        "    #===============================#\n",
        "    def train(self,  training_data, labels):\n",
        "      #assert len(training_data) == len(labels)\n",
        "        #return\n",
        "      m = training_data.shape[0]\n",
        "      #print('before', self.parameters['W'+str(1)])\n",
        "      for i in range(0, self.max_iterations):\n",
        "          \n",
        "          # Forward propagation\n",
        "          self.AL, self.caches = L_model_forward(training_data, self.parameters)      # caches=[(A, W, b), Z] is list of each layer parameters activatio,\n",
        "                                                                                      # weight, bias, and input of the layer Z; \n",
        "                                                                                      # and AL is the output logits\n",
        "          L = len(self.parameters) //2  \n",
        "          delta_L = self.AL - labels\n",
        "          errors = []\n",
        "          errors.append(delta_L)\n",
        "          for l in range(L, 1, -1):\n",
        "              delta = np.dot(delta_L, self.parameters['W'+str(l)]) * gradient(self.caches[l-2][1], 'relu')\n",
        "              errors.append(delta)\n",
        "              delta_L = delta\n",
        "          c = 0\n",
        "          for ll in range(L, 0, -1):\n",
        "              grad_w = np.dot(errors[c].T, self.caches[ll-1][0][0])\n",
        "              grad_b = np.sum(errors[c], axis=0, keepdims=True).T     #.reshape(errors[c].shape[1], 1)\n",
        "              self.parameters['W'+str(ll)] = self.parameters['W'+str(ll)] - self.learning_rate*grad_w\n",
        "              self.parameters['b'+str(ll)] = self.parameters['b'+str(ll)] - self.learning_rate*grad_b\n",
        "              c = c+1\n",
        "          \n",
        "      #print('after', self.parameters['W'+str(1)])\n",
        "                            \n",
        "          # Print the cost every 100 training example\n",
        "     #     if print_cost and i % 100 == 0:\n",
        "     #         print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "\n",
        "    #=========================================#\n",
        "    # Tests the prediction on each element of #\n",
        "    # the testing data. Prints the precision, #\n",
        "    # recall, and accuracy.                   #\n",
        "    #=========================================#\n",
        "    def test(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "        self.logits, _ = L_model_forward(testing_data, self.parameters)      # caches=[(A, W, b), Z] is list of each layer parameters activatio,\n",
        "                                                                                      # weight, bias, and input of the layer Z; \n",
        "                                                                                      # and AL is the output logits\n",
        "        #predictions = np.amax(self.logits, axis=1).reshape(labels.shape[0], 1)\n",
        "        predictions = self.logits.argmax(axis=1).reshape(labels.shape[0], 1)\n",
        "        true_positives = np.sum(np.where(predictions==self.logits, 1, 0))\n",
        "        print(predictions[1:6, :], labels[1:6, :])\n",
        "        #accuracy = true_positives/labels.shape[0]\n",
        "        \n",
        "        print(metrics.classification_report(labels, predictions, labels=[0,1,2,3,4,5,6,7,8,9], digits=4))\n",
        "        "
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5hOn_wk9DP_"
      },
      "source": [
        "### Main method\n",
        "\n",
        "The following cell(s) should complete parts 2.1 to 2.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTl8y9Ao9DP_",
        "outputId": "66a71c9b-be9f-4c74-ac9f-7a7958277213"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  # TODO load training data\n",
        "  # TODO load testing data\n",
        "  train_x, train_y, test_x, test_y = load_data()\n",
        "  m, n_x = train_x.shape\n",
        "  no_outputs=10   # number of classes\n",
        "  train_labels = np.zeros((m, no_outputs))\n",
        "  for i in range(m):\n",
        "    train_labels[i, train_y[i]] = 1\n",
        "  #print(train_labels[1, :], train_y[1, :])\n",
        "\n",
        "  # e.g. net = ANN(0)\n",
        "  no_inputs=n_x; no_hidden_layers=1; hidden_layer_size=28; max_iterations=20;  learning_rate=0.1; \n",
        "  model = ANN(no_inputs, no_hidden_layers, hidden_layer_size, max_iterations, learning_rate, no_outputs)\n",
        "  model.train(train_x, train_labels)\n",
        "  model.test(test_x, test_y)\n",
        "  # TODO call train\n",
        "  # TODO call test"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 784)\n",
            "(28, 1)\n",
            "(10, 28)\n",
            "(10, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]] [[4]\n",
            " [1]\n",
            " [9]\n",
            " [2]\n",
            " [1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000         1\n",
            "           1     0.3333    1.0000    0.5000         3\n",
            "           2     0.0000    0.0000    0.0000         1\n",
            "           3     0.0000    0.0000    0.0000         1\n",
            "           4     0.0000    0.0000    0.0000         2\n",
            "           5     0.0000    0.0000    0.0000         0\n",
            "           6     0.0000    0.0000    0.0000         0\n",
            "           7     0.0000    0.0000    0.0000         0\n",
            "           8     0.0000    0.0000    0.0000         0\n",
            "           9     0.0000    0.0000    0.0000         1\n",
            "\n",
            "   micro avg     0.3333    0.3333    0.3333         9\n",
            "   macro avg     0.0333    0.1000    0.0500         9\n",
            "weighted avg     0.1111    0.3333    0.1667         9\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzSIkjw1_teS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8_d9pKSZ1Ym",
        "outputId": "05b4ed5f-0c31-4d01-fd00-c3c09173aac0"
      },
      "source": [
        "prob = np.array([[0, 0.002, 3], [0, 0.002, 3]])\n",
        "prob  = np.where(prob > 1.0e-10, prob, 1.0e-10)\n",
        "np.log(1.0e-10)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-23.025850929940457"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gznYcAxQdvmq",
        "outputId": "82a582a0-40ac-4ec3-ae9f-a765c766e030"
      },
      "source": [
        "pp = (2,4,5)\n",
        "aa = 3\n",
        "pp1 = (pp, aa)\n",
        "pp1[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe0_0-aXrdzQ",
        "outputId": "59822968-5d38-46bf-9433-8954ccc84922"
      },
      "source": [
        "for i in range(5, 1, -1):\n",
        "  print(i)\n",
        "aa = np.array([2,4,5])\n",
        "np.where(aa<3, 0, 333)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "4\n",
            "3\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0, 333, 333])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvH8q6vkC4XZ",
        "outputId": "db7256e5-73ad-427f-c5c9-cd05358aa56b"
      },
      "source": [
        "prob =np.array([0.002, 0.3]) # np.random.randint(5, size=4) /4\n",
        "print(prob)\n",
        "\n",
        "result = np.where(prob > 0.0000000001, prob, -10)\n",
        "# print(result)\n",
        "np.log(result, out=result, where=result > 0)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.002 0.3  ]\n",
            "[-6.2146081 -1.2039728]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}